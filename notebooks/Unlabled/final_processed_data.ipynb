{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import reduce\n",
    "\n",
    "# Paths\n",
    "base_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs'\n",
    "final_output_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv'\n",
    "\n",
    "# List of files to process\n",
    "cert_files = [\n",
    "    'daily_user_devices_enriched.csv',\n",
    "    'daily_user_email_enriched.csv',\n",
    "    'daily_user_files_enriched.csv',\n",
    "    'daily_user_http_enriched.csv',\n",
    "    'daily_user_logons_enriched.csv',\n",
    "    'RIK_final_training_data.csv'\n",
    "]\n",
    "twos_files = [\n",
    "    'twos_emails_liwc.csv',\n",
    "    'twos_eventviewer_summary.csv',\n",
    "    'twos_keystroke_summary.csv',\n",
    "    'twos_mouse_summary.csv',\n",
    "    'twos_network_summary.csv',\n",
    "    'twos_personality_summary.csv'\n",
    "]\n",
    "\n",
    "all_files = cert_files + twos_files\n",
    "\n",
    "# Columns to encode and drop\n",
    "encode_cols = ['user', 'role', 'business_unit', 'functional_unit', 'department', 'team']\n",
    "drop_cols = ['employee_name', 'email', 'projects', 'supervisor', 'user_id']\n",
    "\n",
    "# Helper: Extract date features\n",
    "def extract_date_features(df, date_col='date_only'):\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        df['day_of_week'] = df[date_col].dt.dayofweek\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['day'] = df[date_col].dt.day\n",
    "        df = df.drop(columns=[date_col])\n",
    "    return df\n",
    "\n",
    "# Read, process, and merge all files\n",
    "dfs = []\n",
    "le_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    path = f\"{base_path}\\\\{file}\"\n",
    "    df = pd.read_csv(path, low_memory=False, dtype=str)  # Read all as string to avoid dtype issues\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in encode_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Extract date features\n",
    "    df = extract_date_features(df)\n",
    "\n",
    "    # Convert all columns except encoded ones to numeric (if possible)\n",
    "    for col in df.columns:\n",
    "        if col not in encode_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Ensure merge keys exist and have consistent types\n",
    "    for key in ['user', 'day_of_week', 'month', 'day']:\n",
    "        if key not in df.columns:\n",
    "            df[key] = pd.NA\n",
    "        df[key] = pd.to_numeric(df[key], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Keep only columns with at least 20% non-missing values (feature coverage filter)\n",
    "    feature_coverage = df.notnull().mean()\n",
    "    keep_cols = [col for col in df.columns if feature_coverage[col] > 0.2 or col in ['user', 'day_of_week', 'month', 'day']]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Only keep numeric columns and merge keys\n",
    "    keep_numeric = [col for col in df.columns if df[col].dtype in ['int64', 'float64', 'Int64'] or col in ['user', 'day_of_week', 'month', 'day']]\n",
    "    df = df[keep_numeric]\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames on merge keys\n",
    "merge_keys = ['user', 'day_of_week', 'month', 'day']\n",
    "def merge_dfs(dfs, keys):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), dfs)\n",
    "\n",
    "final_df = merge_dfs(dfs, merge_keys)\n",
    "\n",
    "# Drop columns with more than 90% missing values after merge\n",
    "missing_thresh = 0.9\n",
    "final_df = final_df.loc[:, final_df.isnull().mean() < missing_thresh]\n",
    "\n",
    "# Fill missing values: float columns with mean, integer columns with median\n",
    "for col in final_df.columns:\n",
    "    if pd.api.types.is_float_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].mean())\n",
    "    elif pd.api.types.is_integer_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# Save final processed data\n",
    "final_df.to_csv(final_output_path, index=False)\n",
    "\n",
    "print(f\"Final processed data saved to: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2958a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final processed data saved to: C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import reduce\n",
    "\n",
    "# Paths\n",
    "base_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs'\n",
    "final_output_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv'\n",
    "\n",
    "# List of files to process\n",
    "cert_files = [\n",
    "    'daily_user_devices_enriched.csv',\n",
    "    'daily_user_email_enriched.csv',\n",
    "    'daily_user_files_enriched.csv',\n",
    "    'daily_user_http_enriched.csv',\n",
    "    'daily_user_logons_enriched.csv',\n",
    "    'RIK_final_training_data.csv'\n",
    "]\n",
    "twos_files = [\n",
    "    'twos_emails_liwc.csv',\n",
    "    'twos_eventviewer_summary.csv',\n",
    "    'twos_keystroke_summary.csv',\n",
    "    'twos_mouse_summary.csv',\n",
    "    'twos_network_summary.csv',\n",
    "    'twos_personality_summary.csv'\n",
    "]\n",
    "\n",
    "all_files = cert_files + twos_files\n",
    "\n",
    "# Columns to encode and drop\n",
    "encode_cols = ['user', 'role', 'business_unit', 'functional_unit', 'department', 'team']\n",
    "drop_cols = ['employee_name', 'email', 'projects', 'supervisor', 'user_id']\n",
    "\n",
    "# Helper: Extract date features\n",
    "def extract_date_features(df, date_col='date_only'):\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        df['day_of_week'] = df[date_col].dt.dayofweek\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['day'] = df[date_col].dt.day\n",
    "        df = df.drop(columns=[date_col])\n",
    "    return df\n",
    "\n",
    "# Read, process, and merge all files\n",
    "dfs = []\n",
    "le_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    path = f\"{base_path}\\\\{file}\"\n",
    "    df = pd.read_csv(path, low_memory=False, dtype=str)  # Read all as string to avoid dtype issues\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in encode_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Extract date features\n",
    "    df = extract_date_features(df)\n",
    "\n",
    "    # Convert all columns except encoded ones to numeric (if possible)\n",
    "    for col in df.columns:\n",
    "        if col not in encode_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Ensure merge keys exist and have consistent types\n",
    "    for key in ['user', 'day_of_week', 'month', 'day']:\n",
    "        if key not in df.columns:\n",
    "            df[key] = pd.NA\n",
    "        # Convert merge keys to int (except 'user' which is already encoded as int)\n",
    "        if key != 'user':\n",
    "            df[key] = pd.to_numeric(df[key], errors='coerce').astype('Int64')\n",
    "        else:\n",
    "            df[key] = pd.to_numeric(df[key], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Keep only numeric columns and merge keys\n",
    "    keep_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64', 'Int64'] or col in ['user', 'day_of_week', 'month', 'day']]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Handle missing values (fill with column mean)\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames on merge keys\n",
    "merge_keys = ['user', 'day_of_week', 'month', 'day']\n",
    "def merge_dfs(dfs, keys):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), dfs)\n",
    "\n",
    "final_df = merge_dfs(dfs, merge_keys)\n",
    "\n",
    "# Drop columns with more than 90% missing values after merge\n",
    "missing_thresh = 0.9\n",
    "final_df = final_df.loc[:, final_df.isnull().mean() < missing_thresh]\n",
    "\n",
    "# Fill missing values: float columns with mean, integer columns with median\n",
    "for col in final_df.columns:\n",
    "    if pd.api.types.is_float_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].mean())\n",
    "    elif pd.api.types.is_integer_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# Save final processed data\n",
    "final_df.to_csv(final_output_path, index=False)\n",
    "\n",
    "print(f\"Final processed data saved to: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611ae41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1464618, 219)\n",
      "Columns: ['user', 'device_events_per_day_x', 'unique_pcs_per_day_x_x', 'unique_filetrees_per_day_x', 'connect_events_per_day_x', 'disconnect_events_per_day_x', 'external_path_access_per_day_x', 'day_of_week', 'month', 'day', 'emails_sent_per_day_x', 'emails_viewed_per_day_x', 'avg_email_size_per_day_x', 'attachments_per_day_x', 'multi_recipient_emails_per_day_x', 'outbound_emails_per_day_x', 'unique_recipients_per_day_x', 'cc_recipients_per_day_x', 'bcc_recipients_per_day_x', 'avg_attachment_size_per_day_x', 'file_events_per_day_x', 'unique_files_per_day_x', 'usb_copies_per_day_x', 'usb_reads_per_day_x', 'file_write_events_x', 'file_open_events_x', 'decoy_file_access_per_day_x', 'web_visits_per_day_x', 'unique_domains_per_day_x', 'unique_paths_per_day_x', 'suspicious_domains_per_day_x', 'file_hosting_visits_per_day_x', 'suspicious_protocol_visits_per_day_x', 'visits_out_of_hours_per_day_x', 'logins_per_day_x', 'unique_pcs_per_day_y_x', 'logins_out_of_hours_x', 'weekend_logins_x', 'earliest_login_time_x', 'latest_login_time_x', 'rolling_7day_avg_logins_x', 'role_x', 'business_unit_x', 'functional_unit_x', 'department_x', 'team_x', 'logins_per_day_y', 'unique_pcs_per_day_x_y', 'logins_out_of_hours_y', 'weekend_logins_y', 'earliest_login_time_y', 'latest_login_time_y', 'rolling_7day_avg_logins_y', 'user_id_x', 'role_y', 'business_unit_y', 'functional_unit_y', 'department_y', 'team_y', 'file_events_per_day_y', 'unique_files_per_day_y', 'usb_copies_per_day_y', 'usb_reads_per_day_y', 'file_write_events_y', 'file_open_events_y', 'decoy_file_access_per_day_y', 'device_events_per_day_y', 'unique_pcs_per_day_y_y', 'unique_filetrees_per_day_y', 'connect_events_per_day_y', 'disconnect_events_per_day_y', 'external_path_access_per_day_y', 'emails_sent_per_day_y', 'emails_viewed_per_day_y', 'avg_email_size_per_day_y', 'attachments_per_day_y', 'multi_recipient_emails_per_day_y', 'outbound_emails_per_day_y', 'unique_recipients_per_day_y', 'cc_recipients_per_day_y', 'bcc_recipients_per_day_y', 'avg_attachment_size_per_day_y', 'web_visits_per_day_y', 'unique_domains_per_day_y', 'unique_paths_per_day_y', 'suspicious_domains_per_day_y', 'file_hosting_visits_per_day_y', 'suspicious_protocol_visits_per_day_y', 'visits_out_of_hours_per_day_y', 'O', 'C', 'E', 'A', 'N', 'id', 'label', 'liwc_WC', 'liwc_Analytic', 'liwc_Clout', 'liwc_Authentic', 'liwc_Tone', 'liwc_WPS', 'liwc_Sixltr', 'liwc_Dic', 'liwc_function', 'liwc_pronoun', 'liwc_ppron', 'liwc_i', 'liwc_we', 'liwc_you', 'liwc_shehe', 'liwc_they', 'liwc_ipron', 'liwc_article', 'liwc_prep', 'liwc_auxverb', 'liwc_adverb', 'liwc_conj', 'liwc_negate', 'liwc_verb', 'liwc_adj', 'liwc_compare', 'liwc_interrog', 'liwc_number', 'liwc_quant', 'liwc_affect', 'liwc_posemo', 'liwc_negemo', 'liwc_anx', 'liwc_anger', 'liwc_sad', 'liwc_social', 'liwc_family', 'liwc_friend', 'liwc_female', 'liwc_male', 'liwc_cogproc', 'liwc_insight', 'liwc_cause', 'liwc_discrep', 'liwc_tentat', 'liwc_certain', 'liwc_differ', 'liwc_percept', 'liwc_see', 'liwc_hear', 'liwc_feel', 'liwc_bio', 'liwc_body', 'liwc_health', 'liwc_sexual', 'liwc_ingest', 'liwc_drives', 'liwc_affiliation', 'liwc_achieve', 'liwc_power', 'liwc_reward', 'liwc_risk', 'liwc_focuspast', 'liwc_focuspresent', 'liwc_focusfuture', 'liwc_relativ', 'liwc_motion', 'liwc_space', 'liwc_time', 'liwc_work', 'liwc_leisure', 'liwc_home', 'liwc_money', 'liwc_relig', 'liwc_death', 'liwc_informal', 'liwc_swear', 'liwc_netspeak', 'liwc_assent', 'liwc_nonflu', 'liwc_filler', 'liwc_AllPunc', 'liwc_Period', 'liwc_Comma', 'liwc_Colon', 'liwc_QMark', 'liwc_Exclam', 'liwc_Dash', 'liwc_Quote', 'liwc_Apostro', 'liwc_Parenth', 'liwc_OtherP', 'date', 'AccessDenied_ratio', 'FileAccess_ratio', 'FileModified_ratio', 'LogInAttempt_ratio', 'LogInFailed_ratio', 'LogInSuccess_ratio', 'LogOff_ratio', 'LogOnSuccess_ratio', 'ProcessStart_ratio', 'total_events', 'unique_event_types', 'key_presses_per_day', 'key_releases_per_day', 'unique_keys_used_per_day', 'session_count_per_day_x', 'avg_keys_per_session', 'session_count_per_day_y', 'mouse_moves_per_day', 'unique_positions_per_day', 'host', 'http_requests_per_day', 'dns_requests_per_day', 'tls_connections_per_day', 'tcp_connections_per_day', 'total_packets_per_day', 'unique_destinations_per_day', 'disagree_count', 'neutral_count', 'agree_count', 'total_questions']\n",
      "Missing values per column:\n",
      " user                             16481\n",
      "device_events_per_day_x        1265625\n",
      "unique_pcs_per_day_x_x         1265625\n",
      "unique_filetrees_per_day_x     1265625\n",
      "connect_events_per_day_x       1265625\n",
      "                                ...   \n",
      "unique_destinations_per_day    1448137\n",
      "disagree_count                 1453976\n",
      "neutral_count                  1453976\n",
      "agree_count                    1453976\n",
      "total_questions                1453976\n",
      "Length: 219, dtype: int64\n",
      "Sample data:\n",
      "    user  device_events_per_day_x  unique_pcs_per_day_x_x  \\\n",
      "0   0.0                      NaN                     NaN   \n",
      "1   0.0                      2.0                     1.0   \n",
      "2   0.0                      2.0                     1.0   \n",
      "3   0.0                      2.0                     1.0   \n",
      "4   0.0                      NaN                     NaN   \n",
      "\n",
      "   unique_filetrees_per_day_x  connect_events_per_day_x  \\\n",
      "0                         NaN                       NaN   \n",
      "1                         2.0                       2.0   \n",
      "2                         2.0                       2.0   \n",
      "3                         2.0                       2.0   \n",
      "4                         NaN                       NaN   \n",
      "\n",
      "   disconnect_events_per_day_x  external_path_access_per_day_x  day_of_week  \\\n",
      "0                          NaN                             NaN          0.0   \n",
      "1                          1.0                             1.0          0.0   \n",
      "2                          1.0                             1.0          0.0   \n",
      "3                          1.0                             1.0          0.0   \n",
      "4                          NaN                             NaN          0.0   \n",
      "\n",
      "   month   day  ...  http_requests_per_day  dns_requests_per_day  \\\n",
      "0    1.0   3.0  ...                    NaN                   NaN   \n",
      "1    1.0   4.0  ...                    NaN                   NaN   \n",
      "2    1.0  10.0  ...                    NaN                   NaN   \n",
      "3    1.0  11.0  ...                    NaN                   NaN   \n",
      "4    1.0  17.0  ...                    NaN                   NaN   \n",
      "\n",
      "   tls_connections_per_day  tcp_connections_per_day  total_packets_per_day  \\\n",
      "0                      NaN                      NaN                    NaN   \n",
      "1                      NaN                      NaN                    NaN   \n",
      "2                      NaN                      NaN                    NaN   \n",
      "3                      NaN                      NaN                    NaN   \n",
      "4                      NaN                      NaN                    NaN   \n",
      "\n",
      "   unique_destinations_per_day  disagree_count  neutral_count  agree_count  \\\n",
      "0                          NaN             NaN            NaN          NaN   \n",
      "1                          NaN             NaN            NaN          NaN   \n",
      "2                          NaN             NaN            NaN          NaN   \n",
      "3                          NaN             NaN            NaN          NaN   \n",
      "4                          NaN             NaN            NaN          NaN   \n",
      "\n",
      "   total_questions  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "\n",
      "[5 rows x 219 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv')\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "print(\"Sample data:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2813006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final processed data saved to: C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import reduce\n",
    "\n",
    "# Paths\n",
    "base_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs'\n",
    "final_output_path = r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv'\n",
    "\n",
    "# List of files to process\n",
    "cert_files = [\n",
    "    'daily_user_devices_enriched.csv',\n",
    "    'daily_user_email_enriched.csv',\n",
    "    'daily_user_files_enriched.csv',\n",
    "    'daily_user_http_enriched.csv',\n",
    "    'daily_user_logons_enriched.csv',\n",
    "    'RIK_final_training_data.csv'\n",
    "]\n",
    "twos_files = [\n",
    "    'twos_emails_liwc.csv',\n",
    "    'twos_eventviewer_summary.csv',\n",
    "    'twos_keystroke_summary.csv',\n",
    "    'twos_mouse_summary.csv',\n",
    "    'twos_network_summary.csv',\n",
    "    'twos_personality_summary.csv'\n",
    "]\n",
    "\n",
    "all_files = cert_files + twos_files\n",
    "\n",
    "# Columns to encode and drop\n",
    "encode_cols = ['user', 'role', 'business_unit', 'functional_unit', 'department', 'team']\n",
    "drop_cols = ['employee_name', 'email', 'projects', 'supervisor', 'user_id']\n",
    "\n",
    "# Helper: Extract date features\n",
    "def extract_date_features(df, date_col='date_only'):\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        df['day_of_week'] = df[date_col].dt.dayofweek\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['day'] = df[date_col].dt.day\n",
    "        df = df.drop(columns=[date_col])\n",
    "    return df\n",
    "\n",
    "# Read, process, and merge all files\n",
    "dfs = []\n",
    "le_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    path = f\"{base_path}\\\\{file}\"\n",
    "    df = pd.read_csv(path, low_memory=False, dtype=str)  # Read all as string to avoid dtype issues\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in encode_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Extract date features\n",
    "    df = extract_date_features(df)\n",
    "\n",
    "    # Convert all columns except encoded ones to numeric (if possible)\n",
    "    for col in df.columns:\n",
    "        if col not in encode_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Ensure merge keys exist and have consistent types\n",
    "    for key in ['user', 'day_of_week', 'month', 'day']:\n",
    "        if key not in df.columns:\n",
    "            df[key] = pd.NA\n",
    "        df[key] = pd.to_numeric(df[key], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Keep only columns with at least 20% non-missing values (feature coverage filter)\n",
    "    feature_coverage = df.notnull().mean()\n",
    "    keep_cols = [col for col in df.columns if feature_coverage[col] > 0.2 or col in ['user', 'day_of_week', 'month', 'day']]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Only keep numeric columns and merge keys\n",
    "    keep_numeric = [col for col in df.columns if df[col].dtype in ['int64', 'float64', 'Int64'] or col in ['user', 'day_of_week', 'month', 'day']]\n",
    "    df = df[keep_numeric]\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames on merge keys\n",
    "merge_keys = ['user', 'day_of_week', 'month', 'day']\n",
    "def merge_dfs(dfs, keys):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=keys, how='outer'), dfs)\n",
    "\n",
    "final_df = merge_dfs(dfs, merge_keys)\n",
    "\n",
    "# Drop columns with more than 90% missing values after merge\n",
    "missing_thresh = 0.9\n",
    "final_df = final_df.loc[:, final_df.isnull().mean() < missing_thresh]\n",
    "\n",
    "# Fill missing values: float columns with mean, integer columns with median\n",
    "for col in final_df.columns:\n",
    "    if pd.api.types.is_float_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].mean())\n",
    "    elif pd.api.types.is_integer_dtype(final_df[col]):\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# Save final processed data\n",
    "final_df.to_csv(final_output_path, index=False)\n",
    "\n",
    "print(f\"Final processed data saved to: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1a7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1464618, 93)\n",
      "Columns: ['user', 'device_events_per_day_x', 'unique_pcs_per_day_x_x', 'unique_filetrees_per_day_x', 'connect_events_per_day_x', 'disconnect_events_per_day_x', 'external_path_access_per_day_x', 'day_of_week', 'month', 'day', 'emails_sent_per_day_x', 'emails_viewed_per_day_x', 'avg_email_size_per_day_x', 'attachments_per_day_x', 'multi_recipient_emails_per_day_x', 'outbound_emails_per_day_x', 'unique_recipients_per_day_x', 'cc_recipients_per_day_x', 'bcc_recipients_per_day_x', 'avg_attachment_size_per_day_x', 'file_events_per_day_x', 'unique_files_per_day_x', 'usb_copies_per_day_x', 'usb_reads_per_day_x', 'file_write_events_x', 'file_open_events_x', 'decoy_file_access_per_day_x', 'web_visits_per_day_x', 'unique_domains_per_day_x', 'unique_paths_per_day_x', 'suspicious_domains_per_day_x', 'file_hosting_visits_per_day_x', 'suspicious_protocol_visits_per_day_x', 'visits_out_of_hours_per_day_x', 'logins_per_day_x', 'unique_pcs_per_day_y_x', 'logins_out_of_hours_x', 'weekend_logins_x', 'earliest_login_time_x', 'latest_login_time_x', 'rolling_7day_avg_logins_x', 'role_x', 'business_unit_x', 'functional_unit_x', 'department_x', 'team_x', 'logins_per_day_y', 'unique_pcs_per_day_x_y', 'logins_out_of_hours_y', 'weekend_logins_y', 'earliest_login_time_y', 'latest_login_time_y', 'rolling_7day_avg_logins_y', 'role_y', 'business_unit_y', 'functional_unit_y', 'department_y', 'team_y', 'file_events_per_day_y', 'unique_files_per_day_y', 'usb_copies_per_day_y', 'usb_reads_per_day_y', 'file_write_events_y', 'file_open_events_y', 'decoy_file_access_per_day_y', 'device_events_per_day_y', 'unique_pcs_per_day_y_y', 'unique_filetrees_per_day_y', 'connect_events_per_day_y', 'disconnect_events_per_day_y', 'external_path_access_per_day_y', 'emails_sent_per_day_y', 'emails_viewed_per_day_y', 'avg_email_size_per_day_y', 'attachments_per_day_y', 'multi_recipient_emails_per_day_y', 'outbound_emails_per_day_y', 'unique_recipients_per_day_y', 'cc_recipients_per_day_y', 'bcc_recipients_per_day_y', 'avg_attachment_size_per_day_y', 'web_visits_per_day_y', 'unique_domains_per_day_y', 'unique_paths_per_day_y', 'suspicious_domains_per_day_y', 'file_hosting_visits_per_day_y', 'suspicious_protocol_visits_per_day_y', 'visits_out_of_hours_per_day_y', 'O', 'C', 'E', 'A', 'N']\n",
      "Missing values per column:\n",
      " user                          0\n",
      "device_events_per_day_x       0\n",
      "unique_pcs_per_day_x_x        0\n",
      "unique_filetrees_per_day_x    0\n",
      "connect_events_per_day_x      0\n",
      "                             ..\n",
      "O                             0\n",
      "C                             0\n",
      "E                             0\n",
      "A                             0\n",
      "N                             0\n",
      "Length: 93, dtype: int64\n",
      "Sample data:\n",
      "    user  device_events_per_day_x  unique_pcs_per_day_x_x  \\\n",
      "0     0                 7.798405                1.057776   \n",
      "1     0                 2.000000                1.000000   \n",
      "2     0                 2.000000                1.000000   \n",
      "3     0                 2.000000                1.000000   \n",
      "4     0                 7.798405                1.057776   \n",
      "\n",
      "   unique_filetrees_per_day_x  connect_events_per_day_x  \\\n",
      "0                    1.996181                  7.798405   \n",
      "1                    2.000000                  2.000000   \n",
      "2                    2.000000                  2.000000   \n",
      "3                    2.000000                  2.000000   \n",
      "4                    1.996181                  7.798405   \n",
      "\n",
      "   disconnect_events_per_day_x  external_path_access_per_day_x  day_of_week  \\\n",
      "0                     3.886941                        3.911464            0   \n",
      "1                     1.000000                        1.000000            0   \n",
      "2                     1.000000                        1.000000            0   \n",
      "3                     1.000000                        1.000000            0   \n",
      "4                     3.886941                        3.911464            0   \n",
      "\n",
      "   month  day  ...  unique_paths_per_day_y  suspicious_domains_per_day_y  \\\n",
      "0      1    3  ...                    39.0                           0.0   \n",
      "1      1    4  ...                    17.0                           0.0   \n",
      "2      1   10  ...                    53.0                           0.0   \n",
      "3      1   11  ...                    39.0                           0.0   \n",
      "4      1   17  ...                    35.0                           0.0   \n",
      "\n",
      "   file_hosting_visits_per_day_y  suspicious_protocol_visits_per_day_y  \\\n",
      "0                            0.0                                   0.0   \n",
      "1                            0.0                                   0.0   \n",
      "2                            0.0                                   0.0   \n",
      "3                            0.0                                   0.0   \n",
      "4                            0.0                                   0.0   \n",
      "\n",
      "   visits_out_of_hours_per_day_y     O     C     E     A     N  \n",
      "0                            4.0  45.0  36.0  33.0  15.0  33.0  \n",
      "1                            3.0  45.0  36.0  33.0  15.0  33.0  \n",
      "2                            1.0  45.0  36.0  33.0  15.0  33.0  \n",
      "3                            6.0  45.0  36.0  33.0  15.0  33.0  \n",
      "4                            9.0  45.0  36.0  33.0  15.0  33.0  \n",
      "\n",
      "[5 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\karun\\OneDrive\\Documents\\RIK\\outputs\\final_outputs\\final_processed_data.csv')\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "print(\"Sample data:\\n\", df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rik-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
